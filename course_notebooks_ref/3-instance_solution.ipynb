{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffef9d0",
   "metadata": {},
   "source": [
    "# Exercise 05: Instance Segmentation\n",
    "\n",
    "So far, we were only interested in `semantic` classes, e.g. foreground / background etc.\n",
    "But in many cases we not only want to know if a certain pixel belongs to a specific class, but also to which unique object (i.e. the task of `instance segmentation`).\n",
    "\n",
    "For isolated objects, this is trivial, all connected foreground pixels form one instance, yet often instances are very close together or even overlapping. Thus we need to think a bit more how to formulate the targets / loss of our network.\n",
    "\n",
    "Furthermore, in instance segmentation the specific value of each label is arbitrary. Here, `Mask 1` and `Mask 2` are equivalently good segmentations even though the values of pixels on individual cells are different.\n",
    "\n",
    "| Image | Mask 1| Mask 2|\n",
    "| :-: | :-: | :-: |\n",
    "| ![image](static/01_instance_img.png) | ![mask1](static/02_instance_teaser.png) | ![mask2](static/03_instance_teaser.png) |\n",
    "\n",
    "Once again: THE SPECIFIC VALUES OF THE LABELS ARE ARBITRARY\n",
    "\n",
    "This means that the model will not be able to learn, if tasked to predict the labels directly.\n",
    "\n",
    "Therefore we split the task of instance segmentation in two and introduce an intermediate target which must be:\n",
    "  1) learnable\n",
    "  2) post-processable into an instance segmentation\n",
    "\n",
    "In this exercise we will go over two common intermediate targets (signed distance transform and affinities),\n",
    "as well as the necessary post-processing for obtaining the final segmentations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9746ca94",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a236c5e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from local import train, NucleiDataset, plot_two, plot_three, plot_four\n",
    "from unet import UNet\n",
    "from tqdm import tqdm\n",
    "import tifffile\n",
    "\n",
    "from skimage.filters import threshold_otsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ec0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"  # 'cuda', 'cpu', 'mps'\n",
    "# make sure gpu is available. Please call a TA if this cell fails\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a custom label color map for showing instances\n",
    "label_cmap = ListedColormap(np.load(\"/group/dl4miacourse/segmentation/cmap_60.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddab646",
   "metadata": {},
   "source": [
    "## Section 1: Signed Distance Transform (SDT)\n",
    "\n",
    "First we will use the signed distance transform as an intermediate learning objective\n",
    "\n",
    "<i>What is the signed distance transform?</i>\n",
    "<br>  - Signed Distance Transform indicates the distance from each specific pixel to the boundary of objects.\n",
    "<br>  - It is positive for pixels inside objects and negative for pixels outside objects (i.e. in the background).\n",
    "<br>  - Remember that deep learning models work best with normalized values, therefore it is important the scale the distance\n",
    "           transform. For simplicity things are often scaled between -1 and 1.\n",
    "<br>  - As an example, here, you see the SDT (right) of the target mask (middle), below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e000d55",
   "metadata": {},
   "source": [
    "![image](static/04_instance_sdt.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703461ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 1.1</b>: Write a function to compute the signed distance transform.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5fab50",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Write a function to calculate SDT.\n",
    "# (Hint: Use `distance_transform_edt`)\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "\n",
    "def compute_sdt(labels: np.ndarray, scale: int):\n",
    "    \"\"\"Function to compute a signed distance transform.\"\"\"\n",
    "    labels = np.asarray(labels)\n",
    "    # compute the distance transform inside of each individual object and the background\n",
    "\n",
    "    # create the signed distance transform\n",
    "\n",
    "    # scale the distances so that they are between -1 and 1 (hint: np.tanh)\n",
    "\n",
    "    # be sure to return your solution as type 'float'\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77611a1",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Write a function to calculate SDT.\n",
    "# (Hint: Use `distance_transform_edt` and `binary_erosion` which are imported in the first cell.)\n",
    "\n",
    "\n",
    "def compute_sdt(labels: np.ndarray, scale: int = 5):\n",
    "    \"\"\"Function to compute a signed distance transform.\"\"\"\n",
    "\n",
    "    # compute the distance transform inside and outside of the objects\n",
    "    labels = np.asarray(labels)\n",
    "    ids = np.unique(labels)\n",
    "    ids = ids[ids != 0]\n",
    "    inner = np.zeros(labels.shape, dtype=np.float32)\n",
    "\n",
    "    for id_ in ids:\n",
    "        inner += distance_transform_edt(labels == id_)\n",
    "    outer = distance_transform_edt(labels == 0)\n",
    "\n",
    "    # create the signed distance transform\n",
    "    distance = inner - outer\n",
    "\n",
    "    # scale the distances so that they are between -1 and 1 (hint: np.tanh)\n",
    "    distance = np.tanh(distance / scale)\n",
    "\n",
    "    # be sure to return your solution as type 'float'\n",
    "    return distance.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303cd09",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Below is a small function to visualize the signed distance transform (SDT). <br> Use it to validate your function.\n",
    "<br> Note that the output of the signed distance transform is not binary, a significant difference from semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd16f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Visualize the signed distance transform using the function you wrote above.\n",
    "root_dir = \"/group/dl4miacourse/segmentation/nuclei_train_data\"  # the directory with all the training samples\n",
    "samples = os.listdir(root_dir)\n",
    "idx = np.random.randint(len(samples))  # take a random sample.\n",
    "img = tifffile.imread(\n",
    "    os.path.join(root_dir, samples[idx], \"image.tif\")\n",
    ")  # get the image\n",
    "label = tifffile.imread(\n",
    "    os.path.join(root_dir, samples[idx], \"label.tif\")\n",
    ")  # get the image\n",
    "sdt = compute_sdt(label)\n",
    "plot_two(img, sdt, label=\"SDT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1733107",
   "metadata": {
    "title": "tags"
   },
   "source": [
    "<b>Questions</b>:\n",
    "1. _Why do we need to normalize the distances between -1 and 1_?\n",
    "\n",
    "\n",
    "2. _What is the effect of changing the scale value? What do you think is a good default value_?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616abb00",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<b>Questions</b>:\n",
    "1. Why do we need to normalize the distances between -1 and 1? <br>\n",
    "  It allows for better targets for the model and enables better training.<br>\n",
    "2. What is the effect of changing the scale value? What do you think is a good default value?<br>\n",
    "  Increasing the scale is equivalent to having a wider boundary region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937b290",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 1.2</b>: <br>\n",
    "    Modify the `SDTDataset` class below to produce the paired raw and SDT images.<br>\n",
    "  1. Use the `compute_sdt` function we just wrote above, to fill in the `create_sdt_target` method below.<br>\n",
    "  2. Modify the `__get_item__` method to return an SDT output rather than a label mask.<br>\n",
    "      - Ensure that all final outputs are of torch tensor type.<br>\n",
    "      - Think about the order in which transformations are applied to the mask/SDT.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDTDataset(Dataset):\n",
    "    \"\"\"A PyTorch dataset to load cell images and nuclei masks.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, img_transform=None, return_mask=False):\n",
    "        self.root_dir = (\n",
    "            \"/group/dl4miacourse/segmentation/\" + root_dir\n",
    "        )  # the directory with all the training samples\n",
    "        self.samples = os.listdir(self.root_dir)  # list the samples\n",
    "        self.return_mask = return_mask\n",
    "        self.transform = (\n",
    "            transform  # transformations to apply to both inputs and targets\n",
    "        )\n",
    "        self.img_transform = img_transform  # transformations to apply to raw image only\n",
    "        #  transformations to apply just to inputs\n",
    "        inp_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),  # 0.5 = mean and 0.5 = variance\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.loaded_imgs = [None] * len(self.samples)\n",
    "        self.loaded_masks = [None] * len(self.samples)\n",
    "        for sample_ind in range(len(self.samples)):\n",
    "            img_path = os.path.join(\n",
    "                self.root_dir, self.samples[sample_ind], \"image.tif\"\n",
    "            )\n",
    "            image = Image.open(img_path)\n",
    "            image.load()\n",
    "            self.loaded_imgs[sample_ind] = inp_transforms(image)\n",
    "            mask_path = os.path.join(\n",
    "                self.root_dir, self.samples[sample_ind], \"label.tif\"\n",
    "            )\n",
    "            mask = Image.open(mask_path)\n",
    "            mask.load()\n",
    "            self.loaded_masks[sample_ind] = mask\n",
    "\n",
    "    # get the total number of samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # fetch the training sample given its index\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        #  TODO: Modify this function to return an image and sdt pair\n",
    "\n",
    "        # we'll be using the Pillow library for reading files\n",
    "        # since many torchvision transforms operate on PIL images\n",
    "        image = self.loaded_imgs[idx]\n",
    "        mask = self.loaded_masks[idx]\n",
    "        if self.transform is not None:\n",
    "            # Note: using seeds to ensure the same random transform is applied to\n",
    "            # the image and mask\n",
    "            seed = torch.seed()\n",
    "            torch.manual_seed(seed)\n",
    "            image = self.transform(image)\n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.transform(mask)\n",
    "        if self.img_transform is not None:\n",
    "            image = self.img_transform(image)\n",
    "        if self.return_mask is True:\n",
    "            # want to be able to compare to true label for validation\n",
    "            return image, transforms.ToTensor()(mask), sdt\n",
    "        else:\n",
    "            # only need the image and sdt for training\n",
    "            return image, sdt\n",
    "\n",
    "    def create_sdt_target(self):\n",
    "        # TODO: Fill in function\n",
    "        # make sure this function is returning a torch tensor\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f750ed0",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "class SDTDataset(Dataset):\n",
    "    \"\"\"A PyTorch dataset to load cell images and nuclei masks.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, img_transform=None, return_mask=False):\n",
    "        self.root_dir = (\n",
    "            \"/group/dl4miacourse/segmentation/\" + root_dir\n",
    "        )  # the directory with all the training samples\n",
    "        self.samples = os.listdir(self.root_dir)  # list the samples\n",
    "        self.return_mask = return_mask\n",
    "        self.transform = (\n",
    "            transform  # transformations to apply to both inputs and targets\n",
    "        )\n",
    "        self.img_transform = img_transform  # transformations to apply to raw image only\n",
    "        #  transformations to apply just to inputs\n",
    "        inp_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),  # 0.5 = mean and 0.5 = variance\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.loaded_imgs = [None] * len(self.samples)\n",
    "        self.loaded_masks = [None] * len(self.samples)\n",
    "        for sample_ind in range(len(self.samples)):\n",
    "            img_path = os.path.join(\n",
    "                self.root_dir, self.samples[sample_ind], \"image.tif\"\n",
    "            )\n",
    "            image = Image.open(img_path)\n",
    "            image.load()\n",
    "            self.loaded_imgs[sample_ind] = inp_transforms(image)\n",
    "            mask_path = os.path.join(\n",
    "                self.root_dir, self.samples[sample_ind], \"label.tif\"\n",
    "            )\n",
    "            mask = Image.open(mask_path)\n",
    "            mask.load()\n",
    "            self.loaded_masks[sample_ind] = mask\n",
    "\n",
    "    # get the total number of samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # fetch the training sample given its index\n",
    "    def __getitem__(self, idx):\n",
    "        # We'll be using the Pillow library for reading files\n",
    "        # since many torchvision transforms operate on PIL images\n",
    "        image = self.loaded_imgs[idx]\n",
    "        mask = self.loaded_masks[idx]\n",
    "        if self.transform is not None:\n",
    "            # Note: using seeds to ensure the same random transform is applied to\n",
    "            # the image and mask\n",
    "            seed = torch.seed()\n",
    "            torch.manual_seed(seed)\n",
    "            image = self.transform(image)\n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.transform(mask)\n",
    "        sdt = self.create_sdt_target(mask)\n",
    "        if self.img_transform is not None:\n",
    "            image = self.img_transform(image)\n",
    "        if self.return_mask is True:\n",
    "            return image, transforms.ToTensor()(mask), sdt\n",
    "        else:\n",
    "            return image, sdt\n",
    "\n",
    "    def create_sdt_target(self, mask):\n",
    "\n",
    "        sdt_target_array = compute_sdt(mask)\n",
    "        sdt_target = transforms.ToTensor()(sdt_target_array)\n",
    "        return sdt_target.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b9a97",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Test your function\n",
    "\n",
    "Next, we will create a training dataset and data loader.\n",
    "We will use `plot_two` (imported in the first cell) to verify that our dataset solution is correct. The output should show 2 images: the raw image and the corresponding SDT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4562c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SDTDataset(\"nuclei_train_data\", transforms.RandomCrop(256))\n",
    "train_loader = DataLoader(train_data, batch_size=5, shuffle=True, num_workers=8)\n",
    "\n",
    "idx = np.random.randint(len(train_data))  # take a random sample\n",
    "img, sdt = train_data[idx]  # get the image and the nuclei masks\n",
    "plot_two(img[0], sdt[0], label=\"SDT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957b928",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 1.3</b>: Train the U-Net.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1e1f7",
   "metadata": {},
   "source": [
    "In this task, initialize the UNet, specify a loss function, learning rate, and optimizer, and train the model.<br>\n",
    "<br> For simplicity we will use a pre-made training function imported from `local.py`. <br>\n",
    "<u>Hints</u>:<br>\n",
    "  - Loss function - [torch losses](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "  - Optimizer - [torch optimizers](https://pytorch.org/docs/stable/optim.html)\n",
    "  - Final Activation - there are a few options (only one is the best)\n",
    "      - [sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)\n",
    "      - [tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh)\n",
    "      - [relu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34229d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model.\n",
    "unet = UNet(\n",
    "    depth=...,\n",
    "    in_channels=...,\n",
    "    num_fmaps=...,\n",
    "    fmap_inc_factor=...,\n",
    "    downsample_factor=...,\n",
    "    padding=...,\n",
    "    final_activation=...,\n",
    "    out_channels=...,\n",
    ")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Choose a loss function.\n",
    "\n",
    "# Choose an optimizer.\n",
    "\n",
    "# Use the train function provided in local.py\n",
    "# to train the model for 20 epochs.\n",
    "\n",
    "for epoch in range(20):\n",
    "    train(\n",
    "        model=...,\n",
    "        loader=...,\n",
    "        optimizer=...,\n",
    "        loss_function=...,\n",
    "        epoch=...,\n",
    "        log_interval=10,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c276ff3d",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "unet = UNet(\n",
    "    depth=4,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    final_activation=\"Tanh\",\n",
    "    num_fmaps=16,\n",
    "    fmap_inc_factor=3,\n",
    "    downsample_factor=2,\n",
    "    padding=\"same\",\n",
    "    upsample_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    train(\n",
    "        unet,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        loss,\n",
    "        epoch,\n",
    "        log_interval=10,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a6452",
   "metadata": {},
   "source": [
    "Now, let's apply our trained model and visualize some random samples. <br>\n",
    "First, we create a validation dataset. <br> Next, we sample a random image from the dataset and input into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f07cb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "val_data = SDTDataset(\"nuclei_val_data\")\n",
    "unet.eval()\n",
    "idx = np.random.randint(len(val_data))  # take a random sample.\n",
    "image, sdt = val_data[idx]  # get the image and the nuclei masks.\n",
    "image = image.to(device)\n",
    "pred = unet(torch.unsqueeze(image, dim=0))\n",
    "image = np.squeeze(image.cpu())\n",
    "sdt = np.squeeze(sdt.cpu().numpy())\n",
    "pred = np.squeeze(pred.cpu().detach().numpy())\n",
    "plot_three(image, sdt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de7f299",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2> Checkpoint 1 </h2>\n",
    "\n",
    "At this point we have a model that does what we told it too, but do not yet have a segmentation. <br>\n",
    "In the next section, we will perform some post-processing and obtain segmentations from our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b0b4c",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Section 2: Post-Processing\n",
    "- See here for a nice overview: [open-cv-image watershed](https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html), although the specifics of our code will be slightly different\n",
    "- Given the distance transform (the output of our model), we first need to find the local maxima that will be used as seed points\n",
    "- The watershed algorithm then expands each seed out in a local \"basin\" until the segments touch or the boundary of the object is hit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b881ab5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 2.1</b>: write a function to find the local maxima of the distance transform\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3ffc7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<u>Hint</u>: Look at the imports. <br>\n",
    "<u>Hint</u>: It is possible to write this function by only adding 2 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label, maximum_filter\n",
    "\n",
    "\n",
    "def find_local_maxima(distance_transform, min_dist_between_points):\n",
    "\n",
    "    # Use `maximum_filter` to perform a maximum filter convolution on the distance_transform\n",
    "\n",
    "    # Uniquely label the local maxima\n",
    "    seeds, n = label(...)\n",
    "\n",
    "    return seeds, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c07b79",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import label, maximum_filter\n",
    "\n",
    "\n",
    "def find_local_maxima(distance_transform, min_dist_between_points):\n",
    "    # Use `maximum_filter` to perform a maximum filter convolution on the distance_transform\n",
    "    max_filtered = maximum_filter(distance_transform, min_dist_between_points)\n",
    "    maxima = max_filtered == distance_transform\n",
    "    # Uniquely label the local maxima\n",
    "    seeds, n = label(maxima)\n",
    "\n",
    "    return seeds, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1873b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your function.\n",
    "from local import test_maximum\n",
    "\n",
    "test_maximum(find_local_maxima)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94996cd2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We now use this function to find the seeds for the watershed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import watershed\n",
    "\n",
    "\n",
    "def watershed_from_boundary_distance(\n",
    "    boundary_distances: np.ndarray,\n",
    "    inner_mask: np.ndarray,\n",
    "    id_offset: float = 0,\n",
    "    min_seed_distance: int = 10,\n",
    "):\n",
    "    \"\"\"Function to compute a watershed from boundary distances.\"\"\"\n",
    "\n",
    "    seeds, n = find_local_maxima(boundary_distances, min_seed_distance)\n",
    "\n",
    "    if n == 0:\n",
    "        return np.zeros(boundary_distances.shape, dtype=np.uint64), id_offset\n",
    "\n",
    "    seeds[seeds != 0] += id_offset\n",
    "\n",
    "    # calculate our segmentation\n",
    "    segmentation = watershed(\n",
    "        boundary_distances.max() - boundary_distances, seeds, mask=inner_mask\n",
    "    )\n",
    "\n",
    "    return segmentation\n",
    "\n",
    "\n",
    "def get_inner_mask(pred, threshold):\n",
    "    inner_mask = pred > threshold\n",
    "    return inner_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e89b69",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 2.2</b>: <br> Use the model to generate a predicted SDT and then use the watershed function we defined above to get post-process into a segmentation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6f331",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(val_data))  # take a random sample\n",
    "image, mask = val_data[idx]  # get the image and the nuclei masks\n",
    "\n",
    "# get the model prediction\n",
    "# Hint: make sure set the model to evaluation\n",
    "# Hint: check the dims of the image, remember they should be [batch, channels, x, y]\n",
    "# Hint: remember to move model outputs to the cpu and check their dimensions (as you did in task 1.3 visualization)\n",
    "pred = ...\n",
    "\n",
    "# Choose a threshold value to use to get the boundary mask\n",
    "thresh = ...\n",
    "\n",
    "# Get inner mask\n",
    "inner_mask = ...\n",
    "\n",
    "# Get the segmentation\n",
    "seg = watershed_from_boundary_distance(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d031e4",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(val_data))  # take a random sample\n",
    "image, mask = val_data[idx]  # get the image and the nuclei masks\n",
    "\n",
    "# Hint: make sure set the model to evaluation\n",
    "unet.eval()\n",
    "\n",
    "image = image.to(device)\n",
    "pred = unet(torch.unsqueeze(image, dim=0))\n",
    "\n",
    "image = np.squeeze(image.cpu())\n",
    "mask = np.squeeze(mask.cpu().numpy())\n",
    "pred = np.squeeze(pred.cpu().detach().numpy())\n",
    "\n",
    "# Choose a threshold value to use to get the boundary mask.\n",
    "# Feel free to play around with the threshold.\n",
    "threshold = threshold_otsu(pred)\n",
    "print(f\"Foreground threshold is {threshold:.3f}\")\n",
    "\n",
    "# Get inner mask\n",
    "inner_mask = get_inner_mask(pred, threshold=threshold)\n",
    "\n",
    "# Get the segmentation\n",
    "seg = watershed_from_boundary_distance(pred, inner_mask, min_seed_distance=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b943235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "\n",
    "plot_four(image, mask, pred, seg, label=\"Target\", cmap=label_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b7a9f",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1. What is the effect of the `min_seed_distance` parameter in watershed?\n",
    "      - Experiment with different values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826bf16",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h2> Checkpoint 2 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5027d8f8",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Section 3: Evaluation\n",
    "Many different evaluation metrics exist, and which one you should use is dependant on the specifics of the data.\n",
    "\n",
    "[This website](https://metrics-reloaded.dkfz.de/problem-category-selection) has a good summary of different options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ba2959",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 3.1</b>: Pick the best metric to use\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5699e",
   "metadata": {},
   "source": [
    "Which of the following should we use for our dataset?:\n",
    "  1) [IoU](https://metrics-reloaded.dkfz.de/metric?id=intersection_over_union)\n",
    "  2) [Accuracy](https://metrics-reloaded.dkfz.de/metric?id=accuracy)\n",
    "  3) [Sensitivity](https://metrics-reloaded.dkfz.de/metric?id=sensitivity) and [Specificity](https://metrics-reloaded.dkfz.de/metric?id=specificity@target_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814fdebb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 3.2</b>: <br> Evaluate metrics for the validation dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182541e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from local import evaluate\n",
    "\n",
    "# Need to re-initialize the dataloader to return masks in addition to SDTs.\n",
    "val_dataset = SDTDataset(\"nuclei_val_data\", return_mask=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "unet.eval()\n",
    "\n",
    "(\n",
    "    precision_list,\n",
    "    recall_list,\n",
    "    accuracy_list,\n",
    ") = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "for idx, (image, mask, sdt) in enumerate(tqdm(val_dataloader)):\n",
    "    image = image.to(device)\n",
    "    pred = unet(image)\n",
    "\n",
    "    image = np.squeeze(image.cpu())\n",
    "    gt_labels = np.squeeze(mask.cpu().numpy())\n",
    "    pred = np.squeeze(pred.cpu().detach().numpy())\n",
    "\n",
    "    # feel free to try different thresholds\n",
    "    thresh = threshold_otsu(pred)\n",
    "\n",
    "    # get boundary mask\n",
    "    inner_mask = get_inner_mask(pred, threshold=thresh)\n",
    "\n",
    "    pred_labels = watershed_from_boundary_distance(\n",
    "        pred, inner_mask, id_offset=0, min_seed_distance=20\n",
    "    )\n",
    "    precision, recall, accuracy = evaluate(gt_labels, pred_labels)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "print(f\"Mean Precision is {np.mean(precision_list):.3f}\")\n",
    "print(f\"Mean Recall is {np.mean(recall_list):.3f}\")\n",
    "print(f\"Mean Accuracy is {np.mean(accuracy_list):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35e612",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Section 4: Affinities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87acc099",
   "metadata": {},
   "source": [
    "<i>What are affinities? </i><br>\n",
    "Here we consider not just the pixel but also its direct neighbors.\n",
    "<br> Imagine there is an edge between two pixels if they are in the same class and no edge if not.\n",
    "<br> If we then take all pixels that are directly and indirectly connected by edges, we get an instance.\n",
    "<br> Essentially, we label edges between neighboring pixels as “connected” or “cut”, rather than labeling the pixels themselves. <br>\n",
    "Here,  we show the (affinity in x + affinity in y) in the bottom right image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96eca0",
   "metadata": {},
   "source": [
    "![image](static/05_instance_affinity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1eda73",
   "metadata": {},
   "source": [
    "Similar to the pipeline used for SDTs, we first need to modify the dataset to produce affinities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c347ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataset for affinities\n",
    "from local import compute_affinities\n",
    "\n",
    "\n",
    "class AffinityDataset(Dataset):\n",
    "    \"\"\"A PyTorch dataset to load cell images and nuclei masks\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, img_transform=None, return_mask=False):\n",
    "        self.root_dir = (\n",
    "            \"/group/dl4miacourse/segmentation/\" + root_dir\n",
    "        )  # the directory with all the training samples\n",
    "        self.samples = os.listdir(self.root_dir)  # list the samples\n",
    "        self.return_mask = return_mask\n",
    "        self.transform = (\n",
    "            transform  # transformations to apply to both inputs and targets\n",
    "        )\n",
    "        self.img_transform = img_transform  # transformations to apply to raw image only\n",
    "        #  transformations to apply just to inputs\n",
    "        inp_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Grayscale(),  # some of the images are RGB\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),  # 0.5 = mean and 0.5 = variance\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.loaded_imgs = [None] * len(self.samples)\n",
    "        self.loaded_masks = [None] * len(self.samples)\n",
    "        for sample_ind in range(len(self.samples)):\n",
    "            img_path = os.path.join(\n",
    "                self.root_dir, self.samples[sample_ind], \"image.tif\"\n",
    "            )\n",
    "            image = Image.open(img_path)\n",
    "            image.load()\n",
    "            self.loaded_imgs[sample_ind] = inp_transforms(image)\n",
    "            mask_path = os.path.join(\n",
    "                self.root_dir, self.samples[sample_ind], \"label.tif\"\n",
    "            )\n",
    "            mask = Image.open(mask_path)\n",
    "            mask.load()\n",
    "            self.loaded_masks[sample_ind] = mask\n",
    "\n",
    "    # get the total number of samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # fetch the training sample given its index\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.loaded_imgs[idx]\n",
    "        mask = self.loaded_masks[idx]\n",
    "        if self.transform is not None:\n",
    "            # Note: using seeds to ensure the same random transform is applied to\n",
    "            # the image and mask\n",
    "            seed = torch.seed()\n",
    "            torch.manual_seed(seed)\n",
    "            image = self.transform(image)\n",
    "            torch.manual_seed(seed)\n",
    "            mask = self.transform(mask)\n",
    "        aff_mask = self.create_aff_target(mask)\n",
    "        if self.img_transform is not None:\n",
    "            image = self.img_transform(image)\n",
    "        if self.return_mask is True:\n",
    "            return image, transforms.ToTensor()(mask), aff_mask\n",
    "        else:\n",
    "            return image, aff_mask\n",
    "\n",
    "    def create_aff_target(self, mask):\n",
    "        aff_target_array = compute_affinities(np.asarray(mask), [[0, 1], [1, 0]])\n",
    "        aff_target = torch.from_numpy(aff_target_array)\n",
    "        return aff_target.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc02a38",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Next we initialize the datasets and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92173bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the datasets\n",
    "\n",
    "train_data = AffinityDataset(\"nuclei_train_data\", transforms.RandomCrop(256))\n",
    "train_loader = DataLoader(train_data, batch_size=5, shuffle=True, num_workers=8)\n",
    "idx = np.random.randint(len(train_data))  # take a random sample\n",
    "img, affinity = train_data[idx]  # get the image and the nuclei masks\n",
    "plot_two(img[0], affinity[0] + affinity[1], label=\"AFFINITY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce327b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 4.1</b>: Train a model with affinities as targets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c5ce5",
   "metadata": {},
   "source": [
    "Repurpose the training loop which you used for the SDTs. <br>\n",
    "Think carefully about your final activation and number of out channels. <br>\n",
    "(The best for SDT is not necessarily the best for affinities.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec3740",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "unet = UNet(\n",
    "    depth=...,\n",
    "    in_channels=...,\n",
    "    downsample_factor=...,\n",
    "    final_activation=...,\n",
    "    out_channels=...,\n",
    ")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# specify loss\n",
    "\n",
    "# specify optimizer\n",
    "\n",
    "# add training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f38bd",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "unet = UNet(\n",
    "    depth=4,\n",
    "    in_channels=1,\n",
    "    num_fmaps=16,\n",
    "    fmap_inc_factor=3,\n",
    "    downsample_factor=2,\n",
    "    padding=\"same\",\n",
    "    upsample_mode=\"nearest\",\n",
    "    final_activation=\"Sigmoid\",  # different from SDTs\n",
    "    out_channels=2,\n",
    ")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# choose a loss function\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(20):\n",
    "    train(\n",
    "        unet,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        loss,\n",
    "        epoch,\n",
    "        log_interval=100,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37a0f5",
   "metadata": {},
   "source": [
    "Let's next look at a prediction on a random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11809c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = AffinityDataset(\"nuclei_val_data\", transforms.RandomCrop(256))\n",
    "val_loader = DataLoader(val_data, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "unet.eval()\n",
    "idx = np.random.randint(len(val_data))  # take a random sample\n",
    "image, mask = val_data[idx]  # get the image and the nuclei masks\n",
    "image = image.to(device)\n",
    "pred = unet(torch.unsqueeze(image, dim=0))\n",
    "image = np.squeeze(image.cpu())\n",
    "mask = np.squeeze(mask.cpu().numpy())\n",
    "pred = np.squeeze(pred.cpu().detach().numpy())\n",
    "\n",
    "plot_three(image, mask[0] + mask[1], pred[0] + pred[1], label=\"Affinity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d727d",
   "metadata": {},
   "source": [
    "Let's also evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673134f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = AffinityDataset(\"nuclei_val_data\", return_mask=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "unet.eval()\n",
    "\n",
    "(\n",
    "    precision_list,\n",
    "    recall_list,\n",
    "    accuracy_list,\n",
    ") = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "for idx, (image, mask, _) in enumerate(tqdm(val_dataloader)):\n",
    "    image = image.to(device)\n",
    "\n",
    "    pred = unet(image)\n",
    "\n",
    "    image = np.squeeze(image.cpu())\n",
    "\n",
    "    gt_labels = np.squeeze(mask.cpu().numpy())\n",
    "\n",
    "    pred = np.squeeze(pred.cpu().detach().numpy())\n",
    "\n",
    "    # feel free to try different thresholds\n",
    "    thresh = threshold_otsu(pred)\n",
    "\n",
    "    # get boundary mask\n",
    "    inner_mask = 0.5 * (pred[0] + pred[1]) > thresh\n",
    "\n",
    "    boundary_distances = distance_transform_edt(inner_mask)\n",
    "\n",
    "    pred_labels = watershed_from_boundary_distance(\n",
    "        boundary_distances, inner_mask, id_offset=0, min_seed_distance=20\n",
    "    )\n",
    "    precision, recall, accuracy = evaluate(gt_labels, pred_labels)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "print(f\"Mean Precision is {np.mean(precision_list):.3f}\")\n",
    "print(f\"Mean Recall is {np.mean(recall_list):.3f}\")\n",
    "print(f\"Mean Accuracy is {np.mean(accuracy_list):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66727408",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Bonus: Further reading on Affinities\n",
    "[Here](https://localshapedescriptors.github.io/) is a blog post describing the Local Shape Descriptor method of instance segmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8cab6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Bonus: Pre-Trained Models\n",
    "Cellpose has an excellent pre-trained model for instance segmentation of cells and nuclei.\n",
    "<br> take a look at the full built-in models and try to apply one to the dataset used in this exercise.\n",
    "<br> -[cellpose github](https://github.com/MouseLand/cellpose)\n",
    "<br> -[cellpose documentation](https://cellpose.readthedocs.io/en/latest/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1dbefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install cellpose.\n",
    "!pip install cellpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed1d26",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from cellpose import models\n",
    "\n",
    "# Implement a cellpose pretrained model.\n",
    "# TODO\n",
    "\n",
    "# evaluation\n",
    "precision_list, recall_list, accuracy_list = [], [], []\n",
    "for idx, (image, mask, _) in enumerate(tqdm(val_loader)):\n",
    "    gt_labels = np.squeeze(mask.cpu().numpy())\n",
    "    image = np.squeeze(image.cpu().numpy())\n",
    "\n",
    "    # evaluate the model to get predictions\n",
    "    pred_labels = ...\n",
    "\n",
    "    precision, recall, accuracy = evaluate(gt_labels, pred_labels[0])\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "print(f\"Mean Precision is {np.mean(precision_list):.3f}\")\n",
    "print(f\"Mean Recall is {np.mean(recall_list):.3f}\")\n",
    "print(f\"Mean Accuracy is {np.mean(accuracy_list):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b69f8",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from cellpose import models\n",
    "\n",
    "model = models.Cellpose(model_type=\"nuclei\")\n",
    "channels = [[0, 0]]\n",
    "\n",
    "precision_list, recall_list, accuracy_list = [], [], []\n",
    "for idx, (image, mask, _) in enumerate(tqdm(val_loader)):\n",
    "    gt_labels = np.squeeze(mask.cpu().numpy())\n",
    "    image = np.squeeze(image.cpu().numpy())\n",
    "    pred_labels, _, _, _ = model.eval([image], diameter=None, channels=channels)\n",
    "\n",
    "    precision, recall, accuracy = evaluate(gt_labels, pred_labels[0])\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "print(f\"Mean Precision is {np.mean(precision_list):.3f}\")\n",
    "print(f\"Mean Recall is {np.mean(recall_list):.3f}\")\n",
    "print(f\"Mean Accuracy is {np.mean(accuracy_list):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
